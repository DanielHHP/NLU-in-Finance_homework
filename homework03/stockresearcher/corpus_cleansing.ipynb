{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 打开投研报告的正确姿势【数据清洗篇】"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在金融投资的战场中，数据是最重要的武器，各家投顾业者精心制作的投研报告往往是我们做决策的重要依据。但是那么多的投研报告该如何看，要如何划重点就考验投资人各自技巧了。在本次课程中。我们人当然是要让深度学习来扮演小助手的角色，好好来学学如何正确研读投研报告。首先第一步当然是要从了解如何从来源清洗数据做起。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "首先引用所有后续可能要使用的Python包，目前版本已经写好两种编码，一种是字层级将文字编码为onehot，第二种是字层级将文字逐字编码为1089维的向量(1*31*31)，也就是根据字形外观做的表征。第三种尚未完成，预计是基于结巴分词与词向量，会是以词汇层级作为编码基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import jieba\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_cuda:False\n"
     ]
    }
   ],
   "source": [
    "is_cuda =torch.cuda.is_available()\n",
    "print('is_cuda:'+str(is_cuda))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "首先要来处理的是之前法给各位过的ResearchTrainSample.json，里面主要是放置各个券商发出的投研报告。由于windows与linux两者对于utf-8的定义不同，各位常常会收到utf-8 bom错误，在此我们可以透过以下语法来进行更换统一为无Bom的utf-8格式(等于于linux定义)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "投研报告共计:\n"
     ]
    }
   ],
   "source": [
    "#更换统一为无Bom的utf-8格式\n",
    "s = codecs.open('data/ResearchTrainSample.json', mode='r', encoding='utf-8-sig').read()\n",
    "codecs.open('data/ResearchTrainSample.json', mode='w', encoding='utf-8').write(s)\n",
    "\n",
    "\n",
    "f = codecs.open('data/ResearchTrainSample.json', mode='r', encoding='utf-8')\n",
    "reports = json.load(f)\n",
    "print('投研报告共计:'.format(len(reports)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc=open('data/ChineseChars.pkl', 'rb') \n",
    "chinesechars = cPickle.load(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabs:4084\n"
     ]
    }
   ],
   "source": [
    "filters = '!,?;；，！？。：:\\r\\n'\n",
    "split = \"|\"\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "vocabs = []\n",
    "for report in reports:\n",
    "    if 'content' in report:\n",
    "        text = report['content'].replace('|', \"\").replace('', '').replace('\\u3000', \"\")\n",
    "        vocabs.extend(sorted(list(set(text))))\n",
    "vocabs = sorted(list(set(vocabs)))\n",
    "vocabs.insert(0, '<unk>')  # 2  unknown\n",
    "vocabs.insert(0, '<eos>')  # 1 end of sentance\n",
    "vocabs.insert(0, '<sos>')  # 0 start ofsentance\n",
    "\n",
    "\n",
    "print('vocabs:{}'.format(len(vocabs)))\n",
    "_idx2char = {i: w for i, w in enumerate(vocabs)}\n",
    "_char2idx = {w: i for i, w in enumerate(vocabs)}\n",
    "\n",
    "\n",
    "def char2idx(charstr):\n",
    "    if charstr in _char2idx:\n",
    "        return _char2idx[charstr]\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "def idx2char(idx):\n",
    "    if idx in _idx2char:\n",
    "        return _idx2char[idx]\n",
    "    else:\n",
    "        return '<unk>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找出潛在標籤\n",
    "fp = codecs.open('data/strategy_tag.txt', 'a', encoding='utf-8')\n",
    "#fp = codecs.open('data/eps.txt', 'a', encoding='utf-8')\n",
    "def prepare_tags():\n",
    "    for report in reports:\n",
    "        if 'content' in report:\n",
    "            text = report['content'].replace('|', \"\").replace('', '').replace('\\u3000', \"\")\n",
    "            translate_map = str.maketrans(filters, split * len(filters))\n",
    "            text = text.translate(translate_map)\n",
    "            seq = text.split(split)\n",
    "            seq = list(filter(None, seq))\n",
    "            for item in seq:\n",
    "                #if 'eps' in str.lower(item) or 'EPS' in item or '每股盈余' in item:\n",
    "                if '评级' in item or '目标价' in item or '买入' in item:\n",
    "                    result.append(report['news_id'] + '\\t' + item + '\\r\\n')\n",
    "                    if len(result) >= 100:\n",
    "                        fp.writelines(result)\n",
    "                        result.clear()\n",
    "    fp.writelines(result)\n",
    "    result.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_methods = ['onehot', 'character', 'word2vec']\n",
    "def sentance2seq(sentance, seg_method='onehot',max_length=50):\n",
    "    results_arr = []\n",
    "    if seg_method == 'onehot':\n",
    "        seq_x = [char2idx(tok) for tok in list(sentance)]\n",
    "        while len(seq_x)<max_length: \n",
    "            seq_x.append(1)\n",
    "        #results_arr = np.eye(len(vocabs), dtype=np.float32)[seq_x]\n",
    "        return seq_x\n",
    "    elif seg_method == 'character':\n",
    "        seq_x = [np.reshape(chinesechars[tok],-1) for tok in list(sentance)]\n",
    "        while len(seq_x)<max_length:\n",
    "            seq_x.append(np.random.standard_normal(1089))\n",
    "        return seq_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class sequence_reader(object):\n",
    "    def __init__(self, file_path='data/ResearchTrainSample.json',max_length=50,is_train=True,is_onehot=False):\n",
    "        '''\n",
    "        負責載入完整end-to-end中文建模需要之語料.\n",
    "        '''\n",
    "        self.file_path = file_path\n",
    "        self.is_train=is_train\n",
    "        self.is_onehot=is_onehot\n",
    "        self.is_train=is_train\n",
    "        self.max_length=max_length\n",
    "        f = codecs.open(self.file_path, mode='r', encoding='utf-8')\n",
    "        reports = json.load(f)\n",
    "        f1 = codecs.open('data/strategy_tag.txt', mode='r', encoding='utf-8')\n",
    "        self.taggeddict = {}\n",
    "\n",
    "        strategies = f1.readlines()\n",
    "        # 將標籤依照newid做規整，存放在對應list中\n",
    "        for strategy in strategies:\n",
    "            if strategy.split('\\t')[0] not in self.taggeddict:\n",
    "                self.taggeddict[strategy.split('\\t')[0]] = []\n",
    "                self.taggeddict[strategy.split('\\t')[0]].append(strategy.split('\\t')[1])\n",
    "            else:\n",
    "                self.taggeddict[strategy.split('\\t')[0]].append(strategy.split('\\t')[1])\n",
    "        #把標注結果放在self.taggeddict\n",
    "        if self.is_train:\n",
    "            self.sequence_lines = [[r['news_id'], r['content'].replace('|', \"\").replace('', '').replace('\\u3000', \"\")]for r in reports if 'content' in r and r['news_id'] in self.taggeddict ]\n",
    "        else:\n",
    "            self.sequence_lines=[[r['news_id'],r['content'].replace('|', \"\").replace('', '').replace('\\u3000', \"\")] for r in reports if 'content' in r ]\n",
    "        self.idx = 0\n",
    "        self.feasures = []\n",
    "        self.labels = []\n",
    "        self.groundtruth = []\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.sequence_lines)\n",
    "    def has_more(self):\n",
    "        if self.idx < self.size() - 1:\n",
    "            return True\n",
    "        return False\n",
    "    def reset(self):\n",
    "        self.idx = 0\n",
    "        np.random.shuffle(self.sequence_lines)\n",
    "\n",
    "    def current_minibatch(self):\n",
    "        return self.feasures\n",
    "\n",
    "    def next_minibatch(self,minibatch_size:int):\n",
    "        global is_onehot\n",
    "        '''\n",
    "\t\tReturn a mini batch of sequence frames and their corresponding ground truth.\n",
    "\t\t'''\n",
    "        batch_x = [] #輸入變數\n",
    "        batch_y = [] #輸出變數\n",
    "        batch_g = [] #ground truth答案\n",
    "\n",
    "        while len(batch_x)<minibatch_size:\n",
    "            \n",
    "            text=self.sequence_lines[self.idx][1]\n",
    "            translate_map = str.maketrans(filters, split * len(filters))\n",
    "            text = text.translate(translate_map)\n",
    "            seqs = text.split(split)\n",
    "            seqs = list(filter(None, seqs))\n",
    "            for seq in seqs:\n",
    "                # 此時已經把文章切成一段一段\n",
    "                #如果新聞id不在標籤列表中\n",
    "                if self.sequence_lines[self.idx][0] not in  self.taggeddict:\n",
    "                        y =[]\n",
    "                        [y.append(0) for i in  range(len(seq))]\n",
    "                        ibatch_g = list(seq)\n",
    "                        while len(y)<self.max_length:\n",
    "                            y.append(0)\n",
    "                            ibatch_g.append('')\n",
    "                        #batch_y.append(np.eye(2, dtype=np.float32)[y])\n",
    "                        if not self.is_onehot:\n",
    "                            batch_y.append(y)\n",
    "                            batch_x.append(sentance2seq(seq))\n",
    "                            batch_g.append(ibatch_g)\n",
    "                        else:\n",
    "                            batch_y.append(np.eye(2, dtype=np.float32)[y])\n",
    "                            batch_x.append(np.eye(len(vocabs), dtype=np.float32)[sentance2seq(seq)])\n",
    "                            batch_g.append(ibatch_g)\n",
    "                # 如果新聞id在標籤列表中\n",
    "                else:\n",
    "                        strategies_set = self.taggeddict[self.sequence_lines[self.idx][0]]\n",
    "                        y = []\n",
    "                        [y.append(0) for i in range(len(seq))]\n",
    "                        ibatch_g = list(seq)\n",
    "                        while len(y)<self.max_length:\n",
    "                            y.append(0)\n",
    "                        for s in strategies_set:\n",
    "                            test=seq.find(s)\n",
    "                            if seq.find(s)!=-1:\n",
    "                                y[seq.find(s):seq.find(s)+len(s)]=1\n",
    "                            ibatch_g.append('')\n",
    "                        if not self.is_onehot:\n",
    "                            batch_y.append(y)\n",
    "                            batch_x.append(sentance2seq(seq))\n",
    "                            batch_g.append(ibatch_g)\n",
    "                        else:\n",
    "                            batch_y.append(np.eye(2, dtype=np.float32)[y])\n",
    "                            batch_x.append(np.eye(len(vocabs), dtype=np.float32)[sentance2seq(seq)])\n",
    "                            batch_g.append(ibatch_g)\n",
    "\n",
    "            self.idx=random.randint(0,len(self.sequence_lines))\n",
    "            if self.idx>len(self.sequence_lines)-1:\n",
    "                self.idx=0\n",
    "            return batch_x, batch_y,batch_g"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "以上就完成了从json中读取投研报告内容，以及透过抽取句子碎片来简化生成标注作业，并完成未来可以让深度学习工具使用的读取器。下面我们就来测一下是否能正确执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', '、', ' ', '业', '绩', '预', '告', '中', '枢', '与', '我', '们', '预', '测', '的', 'E', 'P', 'S', ' ', '0', '.', '7', '5', ' ', '一', '致', '', ''], ['我', '们', '估', '计', '全', '年', '实', '际', '业', '绩', '稍', '超', '出', '预', '期', '的', '可', '能', '性', '更', '大', '', ''], ['2', '、', ' ', '公', '司', '今', '年', '业', '绩', '远', '超', '出', '年', '初', '经', '营', '目', '标', '', ''], ['在', '此', '背', '景', '下', '', ''], ['我', '们', '认', '为', '公', '司', '全', '年', '财', '务', '处', '理', '适', '当', '谨', '慎', '', ''], ['夯', '实', '了', '明', '年', '增', '长', '的', '基', '础', '', ''], ['3', '、', ' ', '公', '司', '单', '四', '季', '度', 'E', 'P', 'S', ' ', '继', '续', '环', '比', '上', '升', '（', '约', '0', '.', '2', '6', ' ', '元', '）', '', ''], ['幅', '度', '达', '到', '3', '0', '%', '左', '右', '', ''], ['主', '要', '受', '合', '资', '公', '司', '新', '蒙', '迪', '欧', '、', 'C', 'X', '-', '5', ' ', '新', '车', '销', '量', '贡', '献', '及', '自', '主', '轿', '车', '销', '售', '进', '入', '旺', '季', '推', '动', '', ''], ['4', '、', ' ', '我', '们', '维', '持', '对', '公', '司', '2', '0', '1', '4', '、', '2', '0', '1', '5', ' ', 'E', 'P', 'S', ' ', '1', '.', '3', '2', '、', '2', '.', '0', '9', ' ', '元', '的', '预', '测', '', ''], ['需', '要', '强', '调', '我', '们', '一', '直', '秉', '持', '适', '当', '谨', '慎', '、', '逐', '渐', '上', '调', '的', '预', '测', '原', '则', '', ''], ['公', '司', '业', '绩', '超', '出', '我', '们', '预', '期', '的', '可', '能', '性', '更', '大', '', ''], ['5', '、', ' ', '市', '场', '明', '显', '低', '估', '了', '公', '司', '已', '达', '到', '的', '（', '主', '力', '车', '型', '）', '盈', '利', '能', '力', '水', '平', '及', '高', '增', '长', '持', '续', '性', '', ''], ['最', '近', '由', '于', '行', '业', '限', '购', '及', '翼', '虎', '召', '回', '提', '供', '了', '绝', '佳', '买', '入', '时', '机', '', ''], ['强', '烈', '建', '议', '买', '入', '', ''], ['目', '标', '价', '1', '8', ' ', '元', '不', '变', '', '']]\n",
      "[[25, 162, 8, 207, 2801, 3844, 714, 219, 1814, 198, 1431, 308, 3844, 2094, 2494, 45, 56, 59, 8, 24, 22, 31, 29, 8, 188, 2983, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1431, 308, 336, 3258, 449, 1229, 1063, 3752, 207, 2801, 2648, 3405, 511, 3844, 1771, 2494, 674, 2941, 1358, 1754, 931, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [26, 162, 8, 451, 681, 291, 1229, 207, 2801, 3497, 3405, 511, 1229, 531, 2786, 3107, 2517, 1842, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [839, 1953, 2925, 1733, 195, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1431, 308, 3261, 225, 451, 681, 449, 1229, 3353, 563, 921, 2397, 3515, 1297, 3336, 1411, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [937, 1063, 253, 1701, 1229, 909, 3708, 2494, 888, 2573, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [27, 162, 8, 451, 681, 611, 820, 1036, 1254, 45, 56, 59, 8, 2800, 2803, 2381, 1977, 194, 604, 4051, 2762, 24, 22, 26, 30, 8, 434, 4052, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1224, 1254, 3481, 538, 27, 24, 13, 1186, 678, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [226, 3236, 660, 686, 3381, 451, 681, 1669, 3121, 3505, 1943, 162, 43, 64, 21, 29, 8, 1669, 3444, 3671, 3615, 3352, 2369, 651, 2980, 226, 3457, 3444, 3671, 772, 3496, 448, 1694, 1036, 1574, 567, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [28, 162, 8, 1431, 308, 2807, 1522, 1095, 451, 681, 26, 24, 25, 28, 162, 26, 24, 25, 29, 8, 45, 56, 59, 8, 25, 22, 27, 26, 162, 26, 22, 24, 33, 8, 434, 2494, 3844, 2094, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [3796, 3236, 1293, 3314, 1431, 308, 188, 2520, 2630, 1522, 3515, 1297, 3336, 1411, 162, 3522, 2154, 194, 3314, 2494, 3844, 2094, 641, 527, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [451, 681, 207, 2801, 3405, 511, 1431, 308, 3844, 1771, 2494, 674, 2941, 1358, 1754, 931, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [29, 162, 8, 1200, 844, 1701, 1716, 345, 336, 253, 451, 681, 1193, 3481, 538, 2494, 4051, 226, 558, 3444, 870, 4052, 2504, 535, 2941, 558, 1999, 1228, 651, 3921, 909, 3708, 1522, 2803, 1358, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1760, 3489, 2438, 260, 3196, 207, 3758, 3364, 651, 2871, 3152, 671, 822, 1579, 368, 253, 2795, 361, 249, 448, 1692, 1780, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1293, 2274, 1267, 3265, 249, 448, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2517, 1842, 312, 25, 32, 8, 434, 197, 661, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "reader=sequence_reader()\n",
    "x,y,g=reader.next_minibatch(2)\n",
    "print(g)\n",
    "print(x)\n",
    "print(y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
